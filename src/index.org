#+TITLE:     The Cluster Model
#+AUTHOR:    VLEAD Systems
#+DATE:      2015-03-25 Wed
#+PROPERTY: session *scratch*
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org
#+OPTIONS: ^nil

* Introduction
  This document describes in detail the model of the systems cluster
  at VLEAD.  The tools, systems, configurations, setup files and
  related discussions are all part of the model.
 
* Prerequisites
  This section enlists the basic knowledge the reader must have to be
  able to comprehend the details of this model. The reader must be
  familiar with the following:
  + Networking basics
  + Ansible 
  + Amazon Web Service (AWS) [EC2, VM creation, console, AMI etc.]
  + OpenVZ 
  + Version control basics
To know further about these please refer [[Appendix][Appendix]].

* The Cluster Model
  The cluster model describes the set of systems which together
  provide the infrastructure for the hosting of the virtual-labs.
  This infrastructure is setup at the following different locations:
  + [[Base4 Cluster][Base4]]
  + [[Base1 Cluster][Base1]]
  + [[Amazon Web Services (AWS) Cluster][AWS]]
  The names of the cluster are given based on the locations in which
  they are deployed. Base1 and Base4 are Centos machines running
  OpenVZ kernel and are physically located in IIIT. 

* Purpose of multiple clusters
** Base4 Cluster
   This cluster is a development cluster.  This cluster is used to
   test new changes made to the develop branch before they are
   considered as stable and merged with the master branch.

** Base1 Cluster 
   Stable commits to the master branch on the base4 cluster are
   released.  Each release has a tag and a version number.  A new
   release is tested on base1 cluster to re-verify its soundness on
   the staging environment, before it is pushed to the production
   environment on AWS.

** Amazon Web Services (AWS) Cluster
   This cluster is the production cluster.  This cluster is modified
   only after changes have been verified on both development (base4)
   and staging (base1) clusters.

   /It should be noted that ADS adapter used on base1 and base4 is/
   /different than adapter used on AWS.  Hence some problems may arise/
   /due to use of different adapter in production in comparison to/
   /staging and development./

   This is the main cluster used by the external virtual-labs users
   across the nation.  

* Bootstrapping of a cluster
  An independent operating system such as a container or a VM or even
  a physical machine is referred as a node.  The collection of such
  related and connected nodes forms a cluster.  Each node in the
  cluster has a specific purpose that it serves.  To setup an entire
  cluster initially, each of the component nodes are setup
  individually in a planned and specific manner.  This process of
  setup of individual nodes in a specific manner to setup a new
  cluster is called the bootstrapping process.  This process is
  tightly related to the provisioning method used on the corresponding
  cluster.
** Provisioning 
  Following provisioning methods have been used on the individual
  clusters:
  - AWS cluster :: AWS cluster is created using VMs as nodes.  These
                   VMs are provisioned using AWS web console, AWS CLI
                   tools or AWS python API
  - Base1 cluster :: Base1 cluster is created using OpenVZ containers
                     which use bridged networking with ethernet
                     interfaces (=--netif_add=).  These containers do
                     not use venet interfaces (=--ipadd=) as venet
                     interfaces do not allow routing to be changed.
                     In base1 cluster we need nodes to have
                     configurable routing so that we can set a custom
                     gateway.  
  - Base4 cluster :: Same as base1

** Bootstrapping the AWS cluster
*** Ansible machine creation
    Create a machine for ansible installation.  Setup rpmfusion-free,
    rpmfusion-nonfree, rpmforge and epel repos on the system.  Use
    #+BEGIN_EXAMPLE
    yum -y install ansible
    #+END_EXAMPLE
    to install ansible

*** Version control setup
    Setup version control on ansible machine using:
    #+BEGIN_EXAMPLE
    yum -y install bzr
    #+END_EXAMPLE

*** Setup basic ansible server
    Setup trust based ssh from ansible_server to itself.  Edit or
    create hosts file to point to correct ansible server.  Ensure that
    run.sh refers to correct hosts file.  
    
    Ensure that roles/common_vars/vars/main.yaml has:
    1. Correct ansible_server_ips.
    2. Private_dns_ips and private_dns_zone are set to none.  

    Use only roles common and ansible to setup ansible server
    directly using ansible.yaml.

*** Setup ossec server
    Create ossec server and setup trust based SSH to it.  Edit hosts
    file to reflect correct IP of ossec-server.  Ensure that
    roles/common_vars/vars/main.yaml has correct ossec_server_ip and
    ossec_client_ips.  Ossec server should be setup with common and
    ossec-server roles only.

*** Enable ossec-client role on ansible server
    Enable ossec-client role on ansible server and other additional
    servers that have been configured earlier.  Ensure that
    roles/common_vars/vars/main.yaml clearly includes ansible server ip
    under ossec_client_ips.

*** Create DNS servers
    Create following servers:
    - public DNS
    - private DNS
    For each of these servers do the following:
    - Copy authorized keys from ansible server to these servers
    - Add server IP in ossec_client_ips in
      roles/common_vars/vars/main.yaml
    - Add server IPs in hosts file 
    
    Specifically for DNS servers look at variables in respective DNS
    server files.  Also ensure that roles/named_server/files has
    correct zone files for each zone with necessary zone_file_prefix.

*** Configure machines to use private DNS
    Edit roles/common_vars/vars/main.yaml and set private_dns_ips and
    private_dns_zone values appropriately.
    
*** Create rsyslog server
    Create a machine to setup rsyslog server.  Configure authorized
    keys from ansible server to this machine.  Add the machine IP to
    hosts file appropriately.  Add server IP to ossec_client_ips list.
    Also add the rsyslog server to IP binding in private and public DNS
    zone files.  Configure rsyslog server with common, ossec_client and
    rsyslog_server roles.

*** Configure machines to send logs to rsyslog server
    Add or uncomment role rsyslog_client on all servers except rsyslog_servers

*** Create machine for configuring reverseproxy
    Note that proper configuration of this and future steps depends
    upon availability of a few test labs.  Hence create a few lab
    VMs/containers as necessary before continuing.  Also update private
    DNS so that lab containers can be resolved using FQDN.

    Create a machine for reverseproxy.  Add its IP to private DNS.  Add
    its IP to ossec_client_IPs.  Configure reverseproxy with roles
    common, ossec_client, rsyslog_client and reverseproxy_server roles.
    Ensure that proxy_domains is set appropriately in
    common_vars/vars/main.yaml file.  Private DNS must be setup
    completely before reverseproxy is created.  
    
    Append following to /etc/httpd/conf/httpd.conf once after
    =NameVirtualHost *:80= line
    #+BEGIN_EXAMPLE
    <VirtualHost *:80>
     ServerAdmin contact@rekallsoftware.com
     DocumentRoot /var/www/html
     ServerName reverseproxy.virtual-labs.ac.in
     ServerAlias reverseproxy.vlabs.ac.in
     ErrorLog logs/reverseproxy.virtual-labs.ac.in-error_log
     CustomLog logs/reverseproxy.virtual-labs.ac.in-access_log common
    </VirtualHost>
    #+END_EXAMPLE
    and restart httpd.  This will help in seeing awstats statics on
    reverseproxy machine.
    
*** Create and configure router
    Create a router machine.  Add its ip in private zone files.  Add it
    to ossec_client_ips.  Ensure that values of following common_vars
    is set appropriately:
    - reverseproxy_ip
    - public_dns_ip
    - local_subnet
    - router_interface_ip
    Add authorized access from ansible server to router
    
    Update public DNS entries such that all requests resolve to router
    public IP except ansible for which there is a dedicated public IP.
    
** Bootstrapping the Base1/Base4 cluster
   - Host machine should have updated CentOS-6.6
   - Host machine should have centos template ready to create
     containers on host machine
   - Note ::  Private ips of the nodes on AWS and base{1,4} clusters
              are same except public IPs of router and ansible nodes.

*** Setup bridged network 
   - Host machine should have updated CentOS-6.6 with vzkernel
     installed and configured properly.
   - Do the steps mentioned in the following link after installing
     required vzkernel
     https://www.sbarjatiya.com/notes_wiki/index.php/Installing_openVZ_on_Cent_OS
**** Connect LAN interface to Bridge (br0) 
     Check your interfaces by running the command
#+BEGIN_EXAMPLE 
   ifonfig -a
#+END_EXAMPLE
     That gives you MAC address of interfaces(eth0, eth1, etc,.). Copy that.
     - Create *ifcfg-br0* file in =/etc/sysconfig/network-scripts/= and
       add the following fields

#+BEGIN_EXAMPLE 
       DEVICE=br0
       BOOTPROTO=dhcp
       ONBOOT=yes
       TYPE=Bridge
       NM_CONTROLLED=no
#+END_EXAMPLE

       - Then create another *ifcfg-eth0* file in
	 =/etc/sysconfig/network-scripts/= and add the following fileds
#+BEGIN_EXAMPLE 
	 DEVICE=eth0
	 HWADDR=<<Hardware Address of eth0 interface>>
	 TYPE=Ethernet
	 ONBOOT=yes
	 NM_CONTROLLED=no
	 BOOTPROTO=none
	 BRIDGE=br0
#+END_EXAMPLE

	 paste the copied MAC address of the interface in HWADDR field
	 - Restart the network using

#+BEGIN_EXAMPLE 
	 service network restart
#+END_EXAMPLE

- Now you can see ip assigned to br0 and you will be able to get
  internet
**** Create bridge(br1) for private Network
    - Create *ifcfg-br1* file in =/etc/sysconfig/network-scripts/= and add
       the following fields
#+BEGIN_EXAMPLE 
       DEVICE=br1
       TYPE=Bridge
       ONBOOT=yes
       NM_CONTROLLED=no
       BOOTPROTO=none
#+END_EXAMPLE
       - Restart network again to see the created bridge(br1)

#+BEGIN_EXAMPLE 
	 service network restart
#+END_EXAMPLE

      - Now you can see the bridges on your machine using *brctl show*
        command. That will you the connections between interfaces.
      - We need to set proxy for all the containers if network uses any proxy

	For more information go through the following links
    https://www.sbarjatiya.com/notes_wiki/index.php/Installing_openVZ_on_Cent_OS
    https://github.com/vlead/ovpl/blob/master/docs/bridge-setup.org

*** Router 
    - We need two IPS to setup router machine one is for accessing
      labs and servers over http, https and dns and another is for
      private network ip. This private ip is for gateway to all the
      nodes and lab containers.
    - Enable ssh access from ansible server to router using key based
      authentication
    - Update public DNS entries such that all requests resolve to
      router public IP except ansible for which there is a dedicated
      public IP.
    - Add its public IP to ossec_client_ips in =common_vars/vars/main.yaml=
    
    - Create router node and configure the network as follows on host
      machine
  
#+BEGIN_EXAMPLE 
(host-machine)$vzctl create 1001 --ostemplate centos-6-x86_64 --hostname router.vlabs.ac.in
#+END_EXAMPLE 

      The router needs to be connected with two bridges br0 and br1 in
      order to get internet connection and to setup private network
      via this router. Run the following commands
#+BEGIN_EXAMPLE 
(host-machine)$vzctl set 1001 --netif_add eth0,,,,br0 --save
(host-machine)$vzctl set 1001 --netif_add eth1,,,,br1 --save
#+END_EXAMPLE 
      It means, the container is created with two interfaces eth0 and
      eth1 and eth0 of the router is connected to br0 of host machine
      and eth1 of the router will be connected to br1.

    - Start the router
#+BEGIN_EXAMPLE 
(host-machine)$vzctl start 1001
#+END_EXAMPLE
    - Enter into the router
#+BEGIN_EXAMPLE 
(host-machine)$vzctl enter 1001
#+END_EXAMPLE
**** Inside the Router 
     Now you may not get internet to the router. To get that do the
     following steps.

     Create *ifcfg-eth0* file in =/etc/sysconfig/network-scripts/= and
     add the following fields. In place of HWADDR add MAC address of
     eth0 interface.
#+BEGIN_EXAMPLE 
DEVICE=eth0
HWADDR=<MAC address of the eth0 interface>
BOOTPROTO=static
ONBOOT=yes
NM_CONTROLLED=no
IPADDR=10.2.58.155
NETMASK=255.255.252.0
GATEWAY=10.2.56.1
DNS1=10.4.12.160
DNS2=10.4.12.221
#+END_EXAMPLE

     Create another =ifcfg-eth1= file in
     =/etc/sysconfig/network-scripts/=.  This one is actually for creating
     private network. It acts like a Gateway to all other containers

#+BEGIN_EXAMPLE 
DEVICE=eth1
HWADDR=<<Hardware address of eth1 interface>>
BOOTPROTO=static
ONBOOT=yes
NM_CONTROLLED=no
IPADDR=10.100.1.1
NETMASK=255.255.252.0
#+END_EXAMPLE
- Restart the router container
- Set proxy 

**** Add  NAT rule and Edit =/etc/sysctl.conf= file
     Write a NATing rule to get internet for all the containers
#+BEGIN_EXAMPLE 
-A POSTROUTING ! -d 10.100.0.0/22 -o eth0 -j SNAT --to-source 10.2.58.154
#+END_EXAMPLE
      Edit/open the file =/etc/sysctl.conf= and modify the following line
#+BEGIN_EXAMPLE 
net.ipv4.ip_forward = 1
#+END_EXAMPLE
- Check the link for above things done for router
https://openvz.org/Using_NAT_for_container_with_private_IPs

*** Config-server node
   - We need two IPs to setup ansible machine one is for public access
     over ssh and another is for private network ip. Private ip is for
     configuring other nodes as well as itself and internal
     connections.
   - Create a machine and configure network with two ips. Do the same
     steps as router node except IPADDR and MAC addresses of the
     interfaces.
   - Add its public ip to ossec_client_ips in
     =common_vars/c=vars/main.yaml=
   
   - Enable ssh access from ansible machine using key-based
     authentication

   - Setup rpmfusion-free, rpmfusion-nonfree, rpmforge and epel repos
     on the system. Use the following command to install ansible
#+BEGIN_EXAMPLE 
     yum -y install ansible
#+END_EXAMPLE
**** Version control setup
    Setup version control on ansible machine using:
#+BEGIN_EXAMPLE
     yum -y install git
#+END_EXAMPLE
    In this node, git command is to pull the updates from master
    branch of the systems model. From this node we do not push the
    updated files/folders to systems-model repository. 

    Clone the systems-model repository. For this we have to add
    public key to repository since we do cloning over ssh by issuing
    the following command.     
#+BEGIN_EXAMPLE 
    git clone git@bitbucket.org:vlead/systems-model.git
#+END_EXAMPLE
**** Share public key to all the nodes     
    In this node we need to generate ssh-keys. We need to copy
    generated ssh-key to all the nodes so that we can run anisble
    play books to configure nodes.
     
    Setup emacs and org-mode-8. Use the link [[https://vlead.virtual-labs.ac.in/wiki/index.php/How_to_setup_emacs_and_org_8][How to setup emacs and
    org 8]].

     Edit or create hosts file to point to correct ansible server.
     Run makefile. It creates build folder with code based on cluster.
     After creating all other nodes we can run site.yaml file to setup
     complete cluster.
*** Create DNS servers
    - Create following servers:
      + public DNS
      + private DNS
    - For each of the above servers do the following:
      + Enable ssh access from ansible using key-based authentication
      + Add server IP in ossec_client_ips in
        roles/common_vars/vars/main.yaml
      + Add server IPs in hosts file 
      + Add DNS entries of all the nodes including lab containers (if
        there are any) in both the DNS servers in
        =common_vars/vars/main.yaml= file where =private_dns_entries=
        and =public_dns_entries= variables are declared.

    - Configure private dns server with roles common, ossec_client,
     rsyslog_client, rsnapshot_client, nagios_client and private_dns server.
    - Configure public dns server with roles common, ossec_client,
      rsyslog_client, rsnapshot_client, nagios_client and public_dns
      server.

    Specifically for DNS servers look at variables in respective DNS
    server files.  Also ensure that roles/named_server/files has
    correct zone files for each zone with necessary zone_file_prefix.

*** Setup ossec server
   - Create ossec server and setup trust based SSH to it. 
   - Edit hosts file to reflect correct IP of ossec-server.
   - Ensure that =roles/common_vars/vars/main.yaml= has correct
     ossec_server_ip and ossec_client_ips. 
   - Enable ssh access from ansible using key-based authentication
   - Configure ossec server with roles common, ossec_client,
     rsyslog_client, rsnapshot_client, nagios_client and
     rsnapshot_server.

*** Enable ossec_client role on all the nodes
    Enable ossec_client role on all the servers and other additional
    servers that have been configured earlier.  Ensure that
    =roles/common_vars/vars/main.yaml= clearly includes all other
    servers ips under ossec_client_ips.


*** Configure machines to use private DNS
    Edit roles/common_vars/vars/main.yaml and set private_dns_ips and
    private_dns_zone values appropriately.

*** Create rsyslog server
    - Create a machine to setup rsyslog server.  
    - Enable ssh access from ansible machine using key-based
      authentication  

    - Add the machine IP to hosts file appropriately.  Add server IP
      to ossec_client_ips list.
    - Configure rsyslog server with roles common, ossec_client,
      rsnapshot_client, nagios_client and rsyslog_server.
    

*** Configure machines to send logs to rsyslog server
    Add or uncomment role rsyslog_client on all servers except rsyslog_servers

*** Create Reverse-proxy server
    Note that proper configuration of this and future steps depends
    upon availability of a few test labs.  Hence create a few lab
    VMs/containers as necessary before continuing.  Also update
    private DNS so that lab containers can be resolved using FQDN.

    - Create a machine for reverseproxy.  
    - Enable ssh access from ansible
    - We need to add ssl certificate to reverseproxy server manually
      before running reverseproxy_server.yaml play book from ansible
      machine
    - Add its IP to private DNS. 
    - Add its IP to ossec_client_IPs in =common_vars/vars/main.yaml= file..  
    - Configure reverseproxy with roles common, ossec_client,
      rsyslog_client, rsnapshot_client, nagios_client and
      reverseproxy_server.
    - Ensure that proxy_domains and awstats_domains are set
      appropriately in =common_vars/vars/main.yaml= file.
    - Private DNS must be setup completely before reverseproxy is
      created.

*** Create Rsnapshot server
    - Rnsapshot server is for taking backup of necessary files and
      folders from required nodes of the cluster.
    - Create a container for rsnapshot server.
    - Add its ip to rsnapshot_server_ips list in =common_vars/vars/main.yaml=
    - Add its ip to ossec_client_ips in =common_vars/vars/main.yaml=
    - Enable ssh access from ansible node using key-based
      authentication.
    - Configure rsnapshot server with roles common, ossec_client,
      rsyslog_client, rsnapshot_client, nagios_client and
      rsnapshot_server.

    - We need to generate ssh-key in this node to perform backup
      operations.  And this generated public key has to be placed in
      all other nodes(nodes which we want to take backup). This tasks
      will be taken care by ansible playbooks of rsanpshot-server and
      rsnapshot-client role.
*** Configure machines to be backed up using rsnapshot server
    Add or uncomment rsnapshot_client role in all the nodes
*** Monitoring Server (Nagios)
    - Create nagios server to monitor services from all the nodes in
      the cluster
    - Enable SSH access from ansible machine 
    - Add its ip to ossec_client_ips list in =common_vars/vars/main.yaml=
    - Configure nagios server with roles common, ossec_client,
      rsyslog_client, rsnapshot_client, nagios_client and
      nagios_server.
    
*** Configure machines to be monitored by Nagios serer
    Add or uncomment nagios_client role in all the nodes

* Design of the cluster model
  The diagram below depicts the architecture of the nodes in the
  cluster model.  This architecture is same for all the three
  deployments (base1, base4 and AWS).  The diagram shows the
  network-setup of the cluster and connectivity between a few key
  nodes.

   #+CAPTION:  Cluster Network Diagram
   #+LABEL:  fig-cluster-network-diagram
   [[./diagrams/overall-cluster-network-diagram.png]]
   
  In this diagram IP1 and IP2 refer to the only two public IP's being
  used by the system.  IP1 is the public IP assigned to router and IP2
  is the public IP assigned to ansible.  Normal virtual-lab users only
  see and contact router.  VLEAD team uses ansible to manage the
  cluster.  In near future ADS interface will also be accessible via
  router, for all virtual-labs developers, perhaps through a different
  port. \\
  This diagram focuses on depicting the important connections in the
  network. There are several other connections which are not shown.
  For example, ansible is connected to every node in the cluster.  All
  the connections are not shown in the diagram. Also there are some
  nodes which are not shown, for example the rsnapshot server, rsyslog
  server, ossec server are not shown. But these nodes are configured
  and are being used in the cluster but are not shown to keep the
  diagram simple.
   
  /In near future an ADS interface will also be accessible via/
  /router, for all the virtual-labs developers, perhaps through a/
  /different port./

* Nodes in the cluster
  There are several nodes in the cluster which are all common to the
  various clusters. The list of nodes is as below:
  
  - [[./rsyslog-server.org][Rsyslog server]]
  - [[./private-dns.org][Private DNS]]
  - [[./public-dns.org][Public DNS]]
  - [[./rp-awstats.org][Reverse Proxy]]
  - [[./nagios-server.org][Nagios server]]
  - [[./router.org][Router]]
  - [[./rsnapshot-server.org][Backup server]]
  - [[./ossec-server.org][OSSEC server]] 

  After the bootstrapping process the nodes in the cluster can be
  brought up using the script below. It is an ansible script which
  will bring up all the nodes one after another with the required
  roles.

#+BEGIN_SRC YAML :tangle site.yaml 
---
#Complete site configuration file
#One yaml file for each server is included here
#Server yaml file matches server FQDN

- include: config_server.yaml

- include: router.yaml

- include: rsnapshot_server.yaml

#- include: ossec_server.yaml

- include: public_dns.yaml

- include: private_dns.yaml

- include: rsyslog_server.yaml

- include: reverseproxy_server.yaml

- include: nagios_server.yaml

#- include: ads_server.yaml


#+END_SRC 
* AWS compliance
  AWS provides a list of security best practices which should be
  followed. These practices help the AWS client take safety
  precautions based on the various features Amazon provides. Using
  these a compliance test of the AWS cluster was done. The following
  file describes the same.

  [[./vlead-compliance-with-aws-security-best-practicses.org][Vlead compliance with aws security best practicses]]

* Known Issues
  A separate repository for systems related issues which includes the
  cluster and its implementation can be viewed [[https://bitbucket.org/vlead/systems/issues?status%3Dnew&status%3Dopen][here]].
* Appendix
** Version control
   The cluster model would involve timely changes, be it in the
   configuration of the nodes or the architecture of the
   cluster. Version control provides a mechanism which would keep
   track of all such changes. Along with tracking changes, it also
   provides the facility to revert back to a previous state of the
   system. The version control system used in this model is "git". To
   know more about git [[https://git-scm.com/][click here]].

** OpenVZ
   OpenVZ is a simple open source kernel level virtualization.  It is
   used to create several isolated containers (Virtual Environments)
   on the same physical resources. Details on why OpenVZ is used can
   be found [[https://openvz.org/Use_cases][here]]. 

   Base1 and Base4 clusters are built using such containers. To know
   more about how to setup containers etc. [[https://openvz.org/User_Guide/Operations_on_Containers][click here]].

** AWS
   AWS stands for Amazon Web Services. It is a proprietary cloud
   service provider. Maintenance and allocation of resources are taken
   care of by the service provider (Amazon). 

   AWS cluster is setup using the Amazon web services. To know more
   about AWS service please read the following :
   + [[https://en.wikipedia.org/wiki/Amazon_Web_Services][Amazon Web services]]
   + [[http://aws.amazon.com/free/][AWS Free Tier]]
   + [[http://aws.amazon.com/ec2/][Amazon EC2]]
   + [[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html][AWS EC2 user guide]]
     
** Networking
   Some basics topics in networking that are required to know are:
   + Bridged networks (using OpenVZ) [[[https://en.wikipedia.org/wiki/Bridging_%2528networking%2529][ Bridging]], [[https://openvz.org/Virtual_Ethernet_device][Bridging in OpenVZ]], [[https://openvz.org/Category:Networking][OpenVZ Networking]] ]
   + Concept of subnets [ [[https://en.wikipedia.org/wiki/Subnetwork][Subnetwork]] ] 
   + Basic knowledge of IP addressing [ [[https://en.wikipedia.org/wiki/IP_address][IP addresing]] ]

** Ansible
   Ansible is a configuration management tool which is used for
   configuring the cluster. It provides remote configuration of nodes
   over SSH. For more details view the following [[https://en.wikipedia.org/wiki/Ansible_%2528software%2529][Ansible]] , [[http://docs.ansible.com/][Ansible
   Documentation]].
* COMMENT Bootstrapping of a cluster deployment
  The cluster is a collection of several nodes.  Each of these nodes
  have a specific purpose that they serve.  To setup the entire
  cluster for the first time we need to setup each of the nodes
  individually.  This process is called the bootstrapping process.
  This process is tightly related to provisioning on corresponding
  cluster. 
** Provisioning vs bootstrapping
   *Provisioning* is a term used to describe the process for bringing up
   a system for setting up the node.\\
   *Bootstrapping* process uses this system to setup the
   nodes. Provisioning is a part of the bootstrapping process, hence
   they cannot be separated.\\
** Bootstrapping of Cluster
   All the nodes of the three deployments are same.  The main
   difference is in the provisioning.  The details of provisioning of
   these clusters is described in the following sections.

*** Provisioning for Base1 and Base4
    The 
# comment Above statement is incomplete
*** Provisioning for AWS
# comment This section needs to be filled    
* COMMENT Difference in the base1 and AWS cluster
  Most of the components of base1 and base4 clusters are same as AWS
  cluster.  The main difference is in the provisioning.  The details of
  provisioning of these clusters is described in this section.

** Provisioning of the base1 and base4 clusters
   *TODO*
** Provisioning of the AWS cluster
   *TODO*


